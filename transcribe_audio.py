"""
Ğ¡ĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸ Ğ°ÑƒĞ´Ğ¸Ğ¾/Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ² Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ OpenAI Whisper.
ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ°Ğ¼ (diarization).

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    python transcribe_audio.py Ğ¿ÑƒÑ‚ÑŒ_Ğº_Ñ„Ğ°Ğ¹Ğ»Ñƒ.mp3
    python transcribe_audio.py Ğ¿ÑƒÑ‚ÑŒ_Ğº_Ñ„Ğ°Ğ¹Ğ»Ñƒ.mp4 --model medium
    python transcribe_audio.py Ğ¿ÑƒÑ‚ÑŒ_Ğº_Ñ„Ğ°Ğ¹Ğ»Ñƒ.wav --diarize
"""

import argparse
import os
import sys
from pathlib import Path


def check_dependencies():
    """ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑĞµÑ‚ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸."""
    missing = []
    
    try:
        import whisper
    except ImportError:
        missing.append("openai-whisper")
    
    try:
        import torch
    except ImportError:
        missing.append("torch")
    
    if missing:
        print("âŒ ĞĞµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ñ‹Ğµ Ğ¿Ğ°ĞºĞµÑ‚Ñ‹:")
        print(f"   pip install {' '.join(missing)}")
        print("\nĞ”Ğ»Ñ GPU-ÑƒÑĞºĞ¾Ñ€ĞµĞ½Ğ¸Ñ (NVIDIA):")
        print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        sys.exit(1)
    
    return True


def transcribe_simple(audio_path: str, model_name: str = "medium", language: str = "ru"):
    """
    ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ¿Ğ¾ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ°Ğ¼.
    
    ĞœĞ¾Ğ´ĞµĞ»Ğ¸ Whisper (Ğ¾Ñ‚ Ğ±Ñ‹ÑÑ‚Ñ€Ğ¾Ğ¹ Ğº Ñ‚Ğ¾Ñ‡Ğ½Ğ¾Ğ¹):
    - tiny: ~1GB VRAM, Ğ±Ñ‹ÑÑ‚Ñ€Ğ°Ñ, Ğ½Ğ¸Ğ·ĞºĞ¾Ğµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾
    - base: ~1GB VRAM
    - small: ~2GB VRAM
    - medium: ~5GB VRAM (Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ)
    - large: ~10GB VRAM, Ğ»ÑƒÑ‡ÑˆĞµĞµ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ¾
    """
    import whisper
    import torch
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ”§ Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {device.upper()}")
    print(f"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ '{model_name}'...")
    
    model = whisper.load_model(model_name, device=device)
    
    print(f"ğŸ¤ Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€ÑƒÑ: {audio_path}")
    print("   Ğ­Ñ‚Ğ¾ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ½ÑÑ‚ÑŒ Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ğ¼Ğ¸Ğ½ÑƒÑ‚...")
    
    result = model.transcribe(
        audio_path,
        language=language,
        verbose=False,
        task="transcribe"
    )
    
    return result


def transcribe_with_diarization(audio_path: str, model_name: str = "medium", language: str = "ru"):
    """
    Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ñ Ñ€Ğ°Ğ·Ğ´ĞµĞ»ĞµĞ½Ğ¸ĞµĞ¼ Ğ¿Ğ¾ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ°Ğ¼ (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ whisperx).
    
    Ğ”Ğ»Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ½ÑƒĞ¶Ğ½Ğ¾:
    1. pip install whisperx
    2. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ğ°ĞºĞºĞ°ÑƒĞ½Ñ‚ Ğ½Ğ° huggingface.co
    3. ĞŸÑ€Ğ¸Ğ½ÑÑ‚ÑŒ ÑƒÑĞ»Ğ¾Ğ²Ğ¸Ñ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ pyannote:
       - https://huggingface.co/pyannote/speaker-diarization
       - https://huggingface.co/pyannote/segmentation
    4. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‚ÑŒ Ñ‚Ğ¾ĞºĞµĞ½: https://huggingface.co/settings/tokens
    5. Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ HF_TOKEN Ğ¸Ğ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ‚ÑŒ Ñ‡ĞµÑ€ĞµĞ· --hf-token
    """
    try:
        import whisperx
        import torch
    except ImportError:
        print("âŒ Ğ”Ğ»Ñ diarization Ğ½ÑƒĞ¶ĞµĞ½ whisperx:")
        print("   pip install whisperx")
        sys.exit(1)
    
    hf_token = os.environ.get("HF_TOKEN")
    if not hf_token:
        print("âŒ ĞĞµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½ HuggingFace Ñ‚Ğ¾ĞºĞµĞ½.")
        print("   Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½ÑƒÑ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ HF_TOKEN")
        print("   Ğ¸Ğ»Ğ¸ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞ¹Ñ‚Ğµ --hf-token YOUR_TOKEN")
        sys.exit(1)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    compute_type = "float16" if device == "cuda" else "int8"
    
    print(f"ğŸ”§ Ğ£ÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {device.upper()}")
    print(f"ğŸ“¥ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ '{model_name}'...")
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¸ Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ
    model = whisperx.load_model(model_name, device, compute_type=compute_type, language=language)
    audio = whisperx.load_audio(audio_path)
    
    print(f"ğŸ¤ Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ±Ğ¸Ñ€ÑƒÑ: {audio_path}")
    result = model.transcribe(audio, batch_size=16)
    
    # Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ
    print("ğŸ“ Ğ’Ñ‹Ñ€Ğ°Ğ²Ğ½Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼...")
    model_a, metadata = whisperx.load_align_model(language_code=language, device=device)
    result = whisperx.align(result["segments"], model_a, metadata, audio, device, return_char_alignments=False)
    
    # Diarization
    print("ğŸ‘¥ ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ¾Ğ²...")
    diarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=device)
    diarize_segments = diarize_model(audio)
    result = whisperx.assign_word_speakers(diarize_segments, result)
    
    return result


def format_output(result, with_diarization=False):
    """Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ² Ñ‡Ğ¸Ñ‚Ğ°ĞµĞ¼Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚."""
    output_lines = []
    
    if with_diarization:
        current_speaker = None
        current_text = []
        
        for segment in result.get("segments", []):
            speaker = segment.get("speaker", "UNKNOWN")
            text = segment.get("text", "").strip()
            
            if speaker != current_speaker:
                if current_text:
                    output_lines.append(f"\n[{current_speaker}]: {' '.join(current_text)}")
                current_speaker = speaker
                current_text = [text] if text else []
            else:
                if text:
                    current_text.append(text)
        
        # ĞŸĞ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ ÑĞ¿Ğ¸ĞºĞµÑ€
        if current_text:
            output_lines.append(f"\n[{current_speaker}]: {' '.join(current_text)}")
    else:
        # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ñ Ñ‚Ğ°Ğ¹Ğ¼ĞºĞ¾Ğ´Ğ°Ğ¼Ğ¸
        for segment in result.get("segments", []):
            start = segment.get("start", 0)
            end = segment.get("end", 0)
            text = segment.get("text", "").strip()
            
            start_time = f"{int(start//60):02d}:{int(start%60):02d}"
            end_time = f"{int(end//60):02d}:{int(end%60):02d}"
            
            output_lines.append(f"[{start_time} - {end_time}] {text}")
    
    return "\n".join(output_lines)


def clean_transcript(text: str) -> str:
    """
    Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ²-Ğ¿Ğ°Ñ€Ğ°Ğ·Ğ¸Ñ‚Ğ¾Ğ².
    Ğ”Ğ»Ñ Ğ±Ğ¾Ğ»ĞµĞµ Ğ³Ğ»ÑƒĞ±Ğ¾ĞºĞ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ»ÑƒÑ‡ÑˆĞµ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ LLM.
    """
    import re
    
    # Ğ¡Ğ»Ğ¾Ğ²Ğ°-Ğ¿Ğ°Ñ€Ğ°Ğ·Ğ¸Ñ‚Ñ‹ Ğ´Ğ»Ñ ÑƒĞ´Ğ°Ğ»ĞµĞ½Ğ¸Ñ
    filler_words = [
        r'\bÑÑÑ+\b', r'\bĞ°Ğ°Ğ°+\b', r'\bĞ¼Ğ¼Ğ¼+\b', r'\bÑƒĞ³Ñƒ\b',
        r'\bĞ½Ñƒ\b', r'\bĞ²Ğ¾Ñ‚\b', r'\bÑ‚Ğ¸Ğ¿Ğ°\b', r'\bĞºĞ¾Ñ€Ğ¾Ñ‡Ğµ\b',
        r'\bĞºĞ°Ğº Ğ±Ñ‹\b', r'\bÑ‚Ğ¾ ĞµÑÑ‚ÑŒ\b', r'\bĞ² Ğ¾Ğ±Ñ‰ĞµĞ¼\b',
        r'\bÑ‚Ğ°Ğº ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ\b', r'\bĞ¿Ğ¾Ğ½Ğ¸Ğ¼Ğ°ĞµÑˆÑŒ\b', r'\bĞ·Ğ½Ğ°Ñ‡Ğ¸Ñ‚\b',
    ]
    
    cleaned = text
    for pattern in filler_words:
        cleaned = re.sub(pattern, '', cleaned, flags=re.IGNORECASE)
    
    # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¼Ğ½Ğ¾Ğ¶ĞµÑÑ‚Ğ²ĞµĞ½Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹
    cleaned = re.sub(r'\s+', ' ', cleaned)
    cleaned = re.sub(r'\s+([.,!?])', r'\1', cleaned)
    
    return cleaned.strip()


def main():
    parser = argparse.ArgumentParser(
        description="Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ Ğ°ÑƒĞ´Ğ¸Ğ¾/Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ Whisper",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹:
  python transcribe_audio.py interview.mp3
  python transcribe_audio.py video.mp4 --model large
  python transcribe_audio.py audio.wav --diarize --hf-token YOUR_TOKEN
  python transcribe_audio.py audio.mp3 --clean
        """
    )
    
    parser.add_argument("audio_file", help="ĞŸÑƒÑ‚ÑŒ Ğº Ğ°ÑƒĞ´Ğ¸Ğ¾/Ğ²Ğ¸Ğ´ĞµĞ¾ Ñ„Ğ°Ğ¹Ğ»Ñƒ")
    parser.add_argument("--model", "-m", default="medium",
                        choices=["tiny", "base", "small", "medium", "large", "large-v2", "large-v3"],
                        help="ĞœĞ¾Ğ´ĞµĞ»ÑŒ Whisper (default: medium)")
    parser.add_argument("--language", "-l", default="ru",
                        help="Ğ¯Ğ·Ñ‹Ğº Ğ°ÑƒĞ´Ğ¸Ğ¾ (default: ru)")
    parser.add_argument("--diarize", "-d", action="store_true",
                        help="Ğ Ğ°Ğ·Ğ´ĞµĞ»Ğ¸Ñ‚ÑŒ Ğ¿Ğ¾ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ°Ğ¼ (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ whisperx)")
    parser.add_argument("--hf-token", help="HuggingFace Ñ‚Ğ¾ĞºĞµĞ½ Ğ´Ğ»Ñ diarization")
    parser.add_argument("--output", "-o", help="Ğ¤Ğ°Ğ¹Ğ» Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°")
    parser.add_argument("--clean", "-c", action="store_true",
                        help="Ğ£Ğ±Ñ€Ğ°Ñ‚ÑŒ ÑĞ»Ğ¾Ğ²Ğ°-Ğ¿Ğ°Ñ€Ğ°Ğ·Ğ¸Ñ‚Ñ‹")
    
    args = parser.parse_args()
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ„Ğ°Ğ¹Ğ»Ğ°
    if not os.path.exists(args.audio_file):
        print(f"âŒ Ğ¤Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {args.audio_file}")
        sys.exit(1)
    
    # Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ñ‚Ğ¾ĞºĞµĞ½Ğ° ĞµÑĞ»Ğ¸ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ğ½
    if args.hf_token:
        os.environ["HF_TOKEN"] = args.hf_token
    
    # ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚ĞµĞ¹
    check_dependencies()
    
    # Ğ¢Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ñ
    try:
        if args.diarize:
            result = transcribe_with_diarization(args.audio_file, args.model, args.language)
        else:
            result = transcribe_simple(args.audio_file, args.model, args.language)
    except Exception as e:
        print(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑĞºÑ€Ğ¸Ğ¿Ñ†Ğ¸Ğ¸: {e}")
        sys.exit(1)
    
    # Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ
    formatted = format_output(result, with_diarization=args.diarize)
    
    # ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ¾Ñ‚ ÑĞ»Ğ¾Ğ²-Ğ¿Ğ°Ñ€Ğ°Ğ·Ğ¸Ñ‚Ğ¾Ğ²
    if args.clean:
        formatted = clean_transcript(formatted)
    
    # Ğ’Ñ‹Ğ²Ğ¾Ğ´/ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ğµ
    if args.output:
        output_path = args.output
    else:
        input_path = Path(args.audio_file)
        output_path = input_path.with_suffix(".txt")
    
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(formatted)
    
    print(f"\nâœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ñ‘Ğ½ Ğ²: {output_path}")
    print(f"ğŸ“„ Ğ Ğ°Ğ·Ğ¼ĞµÑ€: {len(formatted)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²")
    
    # ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ğ¿Ñ€ĞµĞ²ÑŒÑ
    preview = formatted[:500] + "..." if len(formatted) > 500 else formatted
    print(f"\nğŸ“ ĞŸÑ€ĞµĞ²ÑŒÑ:\n{'-'*50}\n{preview}\n{'-'*50}")


if __name__ == "__main__":
    main()
